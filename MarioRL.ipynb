{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RL - Super Mario - Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **_Prerequisites_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "### **install following requirements**\n",
    "______\n",
    "\n",
    "#### _Install gym_super_mario_bros for super mario bros environment setup_\n",
    "> > **pip install gym_super_mario_bros==7.3.0 nes_py**\n",
    "\n",
    "\n",
    "#### _Install pytorch for RL model training_\n",
    "> > **pip install torch==1.10.1 torchvision==0.11.2 torchaudio==0.10.1**\n",
    "\n",
    "\n",
    "#### _Install stable baselines for RL stuff_\n",
    "> > **pip install stable-baselines3**\n",
    "\n",
    "\n",
    "#### _Install for graphical representation_\n",
    "> > **pip install plotly**\n",
    "______\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gym_super_mario_bros nes_py torch torchvision torchaudio stable-baselines3 plotly shimmy \n",
    "# !pip install nes-py\n",
    "# !pip install gym-super-mario-bros==7.3.0\n",
    "# !pip install wheel<0.40.0\n",
    "# !pip install gym==0.21.0\n",
    "# !pip install stable-baselines3==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "# Imports for game setup\n",
    "import gym_super_mario_bros\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.distributions import Categorical\n",
    "import random\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Setting Up Mario-Env**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup game\n",
    "env = gym_super_mario_bros.make('SuperMarioBros2-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Action Space: Discrete(256)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Action Space: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SIMPLE_MOVEMENT Action Space: 7\n",
      "COMPLEX_MOVEMENT Action Space: 12\n"
     ]
    }
   ],
   "source": [
    "print(f\"SIMPLE_MOVEMENT Action Space: {len(SIMPLE_MOVEMENT)}\")\n",
    "print(f\"COMPLEX_MOVEMENT Action Space: {len(COMPLEX_MOVEMENT)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Their are 256 Actions in the Action space of Super Mario Bros. \n",
    "\n",
    "hence it will be difficult to train an AI model with such number of possibles options . To overcome this problem, we can use a SIMPLE_MOVEMENT action space which comprises of only 7 options of operations the model can suggest to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Action Space after reduction: Discrete(12)\n"
     ]
    }
   ],
   "source": [
    "# minimizing the action space / can also be done with two actions move right and jump right\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "print(f\"Total Action Space after reduction: {env.action_space}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:32:04.098 Python[22717:931214] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/jw/lz3gc5915cxgsvv61ql8_83c0000gn/T/com.apple.python3.savedState\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "# Create a flag - restart or not\n",
    "done = True\n",
    "# Loop through each frame in the game\n",
    "for step in range(1000): \n",
    "    # Start the game to begin with \n",
    "    if done: \n",
    "        # Start the gamee\n",
    "        env.reset()\n",
    "    # Do random actions\n",
    "    state, reward, done, info  = env.step(env.action_space.sample())\n",
    "    # Show the game on the screen\n",
    "    env.render()\n",
    "    # time.sleep(0.01)\n",
    "# Close the game\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Preprocess Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import GrayScaleObservation\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation shape: (1, 240, 256, 4)\n",
      "Information: [{'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}]\n",
      "Game End Status: [False]\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the base environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros2-v1')\n",
    "# 2. Simplify the controls \n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "# 3. Grayscale\n",
    "env = GrayScaleObservation(env, keep_dim=True)\n",
    "# 4. Wrap inside the Dummy Environment\n",
    "env = DummyVecEnv([lambda: env])\n",
    "# 5. Stack the frames\n",
    "env = VecFrameStack(env, 4, channels_order='last')\n",
    "\n",
    "\n",
    "state = env.reset()\n",
    "\n",
    "# Taking a random action\n",
    "action = env.action_space.sample()\n",
    "state, reward, done, info = env.step([action])\n",
    "\n",
    "print(\"Observation shape:\", state.shape)\n",
    "print(\"Information:\", info)\n",
    "print(\"Game End Status:\", done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x800 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "for idx in range(state.shape[3]):\n",
    "    plt.subplot(1,4,idx+1)\n",
    "    plt.imshow(state[0][:,:,idx])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Training the RL Models**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  making a training logger to save the model checkpoints after set frequency\n",
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    # making the checkpoint directory\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    # on every step checking if the check_frequency is reached, save the model\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the AI model\n",
    "# PPO Parameters\n",
    "    # policy: Union[str, Type[ActorCriticPolicy]],\n",
    "    # env: Union[GymEnv, str],\n",
    "    # learning_rate: Union[float, Schedule] = 3e-4,\n",
    "    # n_steps: int = 2048,\n",
    "    # batch_size: int = 64,\n",
    "    # n_epochs: int = 10,\n",
    "    # gamma: float = 0.99,\n",
    "    # gae_lambda: float = 0.95,\n",
    "    # clip_range: Union[float, Schedule] = 0.2,\n",
    "    # clip_range_vf: Union[None, float, Schedule] = None,\n",
    "    # normalize_advantage: bool = True,\n",
    "    # ent_coef: float = 0.0,\n",
    "    # vf_coef: float = 0.5,\n",
    "    # max_grad_norm: float = 0.5,\n",
    "    # use_sde: bool = False,\n",
    "    # sde_sample_freq: int = -1,\n",
    "    # target_kl: Optional[float] = None,\n",
    "    # tensorboard_log: Optional[str] = None,\n",
    "    # create_eval_env: bool = False,\n",
    "    # policy_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    # verbose: int = 0,\n",
    "    # seed: Optional[int] = None,\n",
    "    # device: Union[th.device, str] = \"auto\",\n",
    "    # _init_setup_model: bool = True,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting logs dir for tensorboard\n",
    "LOG_DIR = './logs/'\n",
    "# setting the available device for training\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Cnn Policy Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ActorCriticCnnPolicy(\n",
       "  (features_extractor): NatureCNN(\n",
       "    (cnn): Sequential(\n",
       "      (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "      (1): ReLU()\n",
       "      (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "      (3): ReLU()\n",
       "      (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (5): ReLU()\n",
       "      (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (linear): Sequential(\n",
       "      (0): Linear(in_features=46592, out_features=512, bias=True)\n",
       "      (1): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=512, out_features=7, bias=True)\n",
       "  (value_net): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_DIR = './train/'\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "seed = 42\n",
    "model = PPO('CnnPolicy',\n",
    "            env,\n",
    "            learning_rate=0.0001,\n",
    "            n_steps=10240,\n",
    "            batch_size=256,\n",
    "            n_epochs=10,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=1,\n",
    "            seed=seed,\n",
    "            device=device)\n",
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/PPO_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/abdulwadood/Library/Python/3.9/lib/python/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in scalar subtract\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 128   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 79    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 83        |\n",
      "|    iterations           | 2         |\n",
      "|    time_elapsed         | 245       |\n",
      "|    total_timesteps      | 20480     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0199751 |\n",
      "|    clip_fraction        | 0.209     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.93     |\n",
      "|    explained_variance   | -0.00579  |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | 0.545     |\n",
      "|    n_updates            | 10        |\n",
      "|    policy_gradient_loss | -0.00509  |\n",
      "|    value_loss           | 1.88      |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 76         |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 400        |\n",
      "|    total_timesteps      | 30720      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00856624 |\n",
      "|    clip_fraction        | 0.108      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.89      |\n",
      "|    explained_variance   | 0.76       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.948      |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.00531   |\n",
      "|    value_loss           | 4.64       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 72          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013392637 |\n",
      "|    clip_fraction        | 0.146       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.864       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.23        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00118    |\n",
      "|    value_loss           | 5.88        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 735         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012239572 |\n",
      "|    clip_fraction        | 0.187       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.87       |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.376       |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00148    |\n",
      "|    value_loss           | 1.33        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 67          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 911         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009602653 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.85       |\n",
      "|    explained_variance   | 0.976       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.525       |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.000741   |\n",
      "|    value_loss           | 2.21        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 65          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 1087        |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009105818 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.86       |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.243       |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | 0.00146     |\n",
      "|    value_loss           | 1.44        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 64          |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 1267        |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008924645 |\n",
      "|    clip_fraction        | 0.147       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.83       |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 1.84        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | 0.00228     |\n",
      "|    value_loss           | 4.32        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 1442        |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010357222 |\n",
      "|    clip_fraction        | 0.189       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.146       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | 0.00415     |\n",
      "|    value_loss           | 0.979       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 63          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 1610        |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009073801 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.989       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.817       |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -3.71e-05   |\n",
      "|    value_loss           | 4.36        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x326b96880>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=102400, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the final model\n",
    "model.save(\"CnnPolicy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. MlpPolicy Agent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=245760, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=245760, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=7, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHECKPOINT_DIR = './train2/'\n",
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)\n",
    "seed = 42\n",
    "model = PPO('MlpPolicy',\n",
    "            env,\n",
    "            learning_rate=0.0001,\n",
    "            n_steps=10240,\n",
    "            batch_size=256,\n",
    "            n_epochs=10,\n",
    "            tensorboard_log=LOG_DIR,\n",
    "            verbose=1,\n",
    "            seed=seed,\n",
    "            device=device)\n",
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "Logging to ./logs/PPO_5\n",
      "------------------------------\n",
      "| time/              |       |\n",
      "|    fps             | 167   |\n",
      "|    iterations      | 1     |\n",
      "|    time_elapsed    | 61    |\n",
      "|    total_timesteps | 10240 |\n",
      "------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 116         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 175         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004726039 |\n",
      "|    clip_fraction        | 0.00182     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.94       |\n",
      "|    explained_variance   | 0.0259      |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 20.4        |\n",
      "|    n_updates            | 5           |\n",
      "|    policy_gradient_loss | -0.00314    |\n",
      "|    value_loss           | 32          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 107          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 286          |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040706964 |\n",
      "|    clip_fraction        | 0.0098       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.453        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 15.4         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00429     |\n",
      "|    value_loss           | 37.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 103          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 397          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066738985 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.94        |\n",
      "|    explained_variance   | 0.524        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 3.87         |\n",
      "|    n_updates            | 15           |\n",
      "|    policy_gradient_loss | -0.00271     |\n",
      "|    value_loss           | 17.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 100         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 510         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006807059 |\n",
      "|    clip_fraction        | 0.0174      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.591       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 8.31        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00335    |\n",
      "|    value_loss           | 16.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 99          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 618         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.001784262 |\n",
      "|    clip_fraction        | 0.00041     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.92       |\n",
      "|    explained_variance   | 0.586       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 17.3        |\n",
      "|    n_updates            | 25          |\n",
      "|    policy_gradient_loss | -0.00167    |\n",
      "|    value_loss           | 38.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 97           |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 737          |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0060002087 |\n",
      "|    clip_fraction        | 0.0133       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | 0.662        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 7.03         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00177     |\n",
      "|    value_loss           | 16           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 96           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 851          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070413454 |\n",
      "|    clip_fraction        | 0.0321       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.9         |\n",
      "|    explained_variance   | 0.668        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 11.4         |\n",
      "|    n_updates            | 35           |\n",
      "|    policy_gradient_loss | -0.00179     |\n",
      "|    value_loss           | 21.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 94          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 971         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008804703 |\n",
      "|    clip_fraction        | 0.034       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.707       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 3.23        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00246    |\n",
      "|    value_loss           | 16.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 93         |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 1093       |\n",
      "|    total_timesteps      | 102400     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00534352 |\n",
      "|    clip_fraction        | 0.0213     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.86      |\n",
      "|    explained_variance   | 0.688      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 11.5       |\n",
      "|    n_updates            | 45         |\n",
      "|    policy_gradient_loss | -0.000681  |\n",
      "|    value_loss           | 19.2       |\n",
      "----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x3b3dd9a60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=102400, callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the final model\n",
    "model.save(\"MlpPolicy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. CNN Agent (Custom)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(4, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc1 = nn.Linear(26*28*64, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.policy_head = nn.Linear(128, num_actions)\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = nn.functional.relu(self.conv3(x))\n",
    "        x = x.reshape(x.size(0), -1) # Flatten the tensor\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = nn.functional.relu(self.fc3(x))\n",
    "        policy_logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return policy_logits, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomCNN(\n",
       "  (conv1): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=46592, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (policy_head): Linear(in_features=128, out_features=12, bias=True)\n",
       "  (value_head): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming CustomCNN and env are defined and initialized\n",
    "model = CustomCNN(num_actions=env.action_space.n).to(device=device)\n",
    "optimizer = Adam(model.parameters(), lr=0.000001)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: Total Reward = [-93.], Loss = 20.1188: : 1841it [00:10, 177.55it/s]\n",
      "Episode 2: Total Reward = [-1.], Loss = 4.5736: : 2000it [00:10, 196.53it/s]                        \n",
      "Episode 3: Total Reward = [-97.], Loss = 45.1005: : 2000it [00:10, 197.88it/s]                        \n",
      "Episode 4: Total Reward = [-111.], Loss = 38.1171: : 1899it [00:09, 196.91it/s]                       \n",
      "Episode 5: Total Reward = [-102.], Loss = 34.1829: : 1877it [00:09, 197.70it/s]                       \n",
      "Episode 6: Total Reward = [97.], Loss = -29.7414: : 2000it [00:10, 196.15it/s]                        \n",
      "Episode 7: Total Reward = [-63.], Loss = 31.2623: : 2000it [00:10, 190.70it/s]                        \n",
      "Episode 8: Total Reward = [-114.], Loss = 33.3134: : 2000it [00:10, 197.93it/s]                        \n",
      "Episode 9: Total Reward = [-16.], Loss = -6.9522: : 2000it [00:10, 197.09it/s]                        \n",
      "Episode 10: Total Reward = [-82.], Loss = 23.9595: : 2000it [00:10, 198.68it/s]                        \n",
      "Episode 11: Total Reward = [23.], Loss = -13.6027: : 2000it [00:11, 172.64it/s]                        \n",
      "Episode 12: Total Reward = [-152.], Loss = 38.0531: : 1913it [00:10, 175.31it/s]                       \n",
      "Episode 13: Total Reward = [110.], Loss = -12.6294: : 2000it [00:10, 191.95it/s]                        \n",
      "Episode 14: Total Reward = [63.], Loss = -11.0582: : 2000it [00:10, 197.75it/s]                        \n",
      "Episode 15: Total Reward = [-41.], Loss = 9.4039: : 2000it [00:09, 205.09it/s]                        \n",
      "Episode 16: Total Reward = [-51.], Loss = 14.0133: : 2000it [00:10, 196.76it/s]                        \n",
      "Episode 17: Total Reward = [-52.], Loss = 26.1070: : 2000it [00:11, 179.16it/s]                        \n",
      "Episode 18: Total Reward = [-68.], Loss = 29.7864: : 2000it [00:10, 188.13it/s]                        \n",
      "Episode 19: Total Reward = [-30.], Loss = 9.3232: : 2000it [00:10, 192.21it/s]                        \n",
      "Episode 20: Total Reward = [525.], Loss = -129.9887: : 2000it [00:10, 188.72it/s]                        \n",
      "Episode 21: Total Reward = [-148.], Loss = 26.1603: : 2000it [00:10, 185.69it/s]                        \n",
      "Episode 22: Total Reward = [-113.], Loss = 36.3120: : 2000it [00:10, 197.72it/s]                        \n",
      "Episode 23: Total Reward = [-104.], Loss = 45.2665: : 1885it [00:09, 190.40it/s]                       \n",
      "Episode 24: Total Reward = [-145.], Loss = 27.2164: : 1959it [00:10, 190.41it/s]                       \n",
      "Episode 25: Total Reward = [-84.], Loss = 18.6477: : 2000it [00:10, 192.91it/s]                        \n",
      "Episode 26: Total Reward = [-121.], Loss = 20.7829: : 1907it [00:10, 190.11it/s]                       \n",
      "Episode 27: Total Reward = [-111.], Loss = 33.9115: : 2000it [00:10, 190.23it/s]                        \n",
      "Episode 28: Total Reward = [-81.], Loss = 29.4342: : 2000it [00:10, 194.39it/s]                        \n",
      "Episode 29: Total Reward = [-37.], Loss = 20.1281: : 2000it [00:10, 197.63it/s]                        \n",
      "Episode 30: Total Reward = [-123.], Loss = 51.0926: : 2000it [00:10, 185.20it/s]                        \n",
      "Episode 31: Total Reward = [-77.], Loss = 21.8007: : 2000it [00:10, 190.34it/s]                        \n",
      "Episode 32: Total Reward = [153.], Loss = -37.8818: : 2000it [00:10, 196.05it/s]                        \n",
      "Episode 33: Total Reward = [209.], Loss = -55.7655: : 2000it [00:10, 188.29it/s]                        \n",
      "Episode 34: Total Reward = [-82.], Loss = 17.3880: : 2000it [00:11, 178.07it/s]                        \n",
      "Episode 35: Total Reward = [-130.], Loss = 24.9655: : 2000it [00:10, 193.92it/s]                        \n",
      "Episode 36: Total Reward = [-64.], Loss = 8.5453: : 1783it [00:08, 207.60it/s]                       \n",
      "Episode 37: Total Reward = [11.], Loss = -5.8727: : 2000it [00:10, 181.92it/s]                        \n",
      "Episode 38: Total Reward = [-134.], Loss = 26.2450: : 1879it [00:11, 169.87it/s]                       \n",
      "Episode 39: Total Reward = [-80.], Loss = 14.0702: : 2000it [00:12, 158.06it/s]                        \n",
      "Episode 40: Total Reward = [448.], Loss = -113.6349: : 2000it [00:12, 166.14it/s]                        \n",
      "Episode 41: Total Reward = [356.], Loss = -92.7083: : 2000it [00:12, 162.90it/s]                        \n",
      "Episode 42: Total Reward = [-56.], Loss = 15.5720: : 2000it [00:11, 166.77it/s]                        \n",
      "Episode 43: Total Reward = [-65.], Loss = 25.2124: : 2000it [00:10, 187.45it/s]                        \n",
      "Episode 44: Total Reward = [-111.], Loss = 25.2191: : 2000it [00:11, 168.43it/s]                        \n",
      "Episode 45: Total Reward = [-80.], Loss = 18.1287: : 2000it [00:11, 180.53it/s]                        \n",
      "Episode 46: Total Reward = [57.], Loss = -27.7647: : 2000it [00:11, 180.73it/s]                        \n",
      "Episode 47: Total Reward = [-97.], Loss = 23.7956: : 1877it [00:10, 174.62it/s]                       \n",
      "Episode 48: Total Reward = [369.], Loss = -93.7946: : 2000it [00:11, 173.13it/s]                        \n",
      "Episode 49: Total Reward = [-109.], Loss = 39.3133: : 2000it [00:10, 188.51it/s]                        \n",
      "Episode 50: Total Reward = [-180.], Loss = 36.3212: : 1983it [00:11, 176.03it/s]                       \n",
      "Episode 51: Total Reward = [158.], Loss = -42.0749: : 2000it [00:11, 176.60it/s]                        \n",
      "Episode 52: Total Reward = [350.], Loss = -87.7710: : 2000it [00:10, 184.58it/s]                        \n",
      "Episode 53: Total Reward = [-82.], Loss = -0.4280: : 1843it [00:10, 175.48it/s]                       \n",
      "Episode 54: Total Reward = [-52.], Loss = 9.2332: : 2000it [00:11, 177.65it/s]                        \n",
      "Episode 55: Total Reward = [-164.], Loss = 21.8618: : 1945it [00:10, 182.10it/s]                       \n",
      "Episode 56: Total Reward = [-8.], Loss = -13.5849: : 2000it [00:11, 177.42it/s]                        \n",
      "Episode 57: Total Reward = [-55.], Loss = 15.4986: : 2000it [00:10, 182.71it/s]                        \n",
      "Episode 58: Total Reward = [-74.], Loss = 18.4505: : 2000it [00:11, 177.73it/s]                        \n",
      "Episode 59: Total Reward = [-57.], Loss = 21.0080: : 2000it [00:10, 184.09it/s]                        \n",
      "Episode 60: Total Reward = [-29.], Loss = 2.1879: : 2000it [00:11, 177.81it/s]                        \n",
      "Episode 61: Total Reward = [-53.], Loss = 17.7454: : 2000it [00:10, 182.63it/s]                        \n",
      "Episode 62: Total Reward = [-159.], Loss = 24.7251: : 1923it [00:10, 186.44it/s]                       \n",
      "Episode 63: Total Reward = [-129.], Loss = 18.9107: : 1911it [00:10, 184.13it/s]                       \n",
      "Episode 64: Total Reward = [-37.], Loss = 19.7319: : 2000it [00:10, 188.56it/s]                        \n",
      "Episode 65: Total Reward = [-64.], Loss = 12.0701: : 2000it [00:11, 175.99it/s]                        \n",
      "Episode 66: Total Reward = [-19.], Loss = 4.2584: : 2000it [00:11, 171.16it/s]                        \n",
      "Episode 67: Total Reward = [-61.], Loss = 14.7163: : 2000it [00:10, 183.80it/s]                        \n",
      "Episode 68: Total Reward = [-67.], Loss = 20.4733: : 2000it [00:11, 181.27it/s]                        \n",
      "Episode 69: Total Reward = [-121.], Loss = 37.2555: : 2000it [00:11, 169.76it/s]                        \n",
      "Episode 70: Total Reward = [-69.], Loss = 31.8747: : 2000it [00:10, 186.40it/s]                        \n",
      "Episode 71: Total Reward = [-71.], Loss = 13.9226: : 2000it [00:10, 189.87it/s]                        \n",
      "Episode 72: Total Reward = [-115.], Loss = 49.3839: : 2000it [00:11, 179.14it/s]                        \n",
      "Episode 73: Total Reward = [-59.], Loss = 3.6678: : 1789it [00:09, 178.93it/s]                       \n",
      "Episode 74: Total Reward = [-101.], Loss = 31.0792: : 1861it [00:10, 181.86it/s]                       \n",
      "Episode 75: Total Reward = [185.], Loss = -30.3690: : 2000it [00:11, 181.51it/s]                        \n",
      "Episode 76: Total Reward = [-78.], Loss = 22.6366: : 2000it [00:11, 181.73it/s]                        \n",
      "Episode 77: Total Reward = [-59.], Loss = 14.0439: : 2000it [00:11, 181.34it/s]                        \n",
      "Episode 78: Total Reward = [-154.], Loss = 49.4970: : 1923it [00:10, 185.47it/s]                       \n",
      "Episode 79: Total Reward = [-58.], Loss = 14.6269: : 2000it [00:10, 189.98it/s]                        \n",
      "Episode 80: Total Reward = [-106.], Loss = 11.7710: : 2000it [00:11, 177.68it/s]                        \n",
      "Episode 81: Total Reward = [19.], Loss = -5.3574: : 2000it [00:10, 182.28it/s]                        \n",
      "Episode 82: Total Reward = [186.], Loss = -43.8599: : 2000it [00:11, 181.60it/s]                        \n",
      "Episode 83: Total Reward = [-72.], Loss = 39.3664: : 2000it [00:10, 181.92it/s]                        \n",
      "Episode 84: Total Reward = [-104.], Loss = 34.9188: : 2000it [00:10, 187.51it/s]                        \n",
      "Episode 85: Total Reward = [-68.], Loss = 16.8617: : 1815it [00:10, 177.63it/s]                       \n",
      "Episode 86: Total Reward = [-37.], Loss = 17.3765: : 2000it [00:11, 176.29it/s]                        \n",
      "Episode 87: Total Reward = [-56.], Loss = 18.7086: : 2000it [00:11, 171.94it/s]                        \n",
      "Episode 88: Total Reward = [-76.], Loss = 5.6014: : 1825it [00:09, 196.99it/s]                       \n",
      "Episode 89: Total Reward = [12.], Loss = -15.8455: : 2000it [00:10, 183.32it/s]                        \n",
      "Episode 90: Total Reward = [157.], Loss = -43.0172: : 2000it [00:10, 187.47it/s]                        \n",
      "Episode 91: Total Reward = [-75.], Loss = 8.8804: : 2000it [00:10, 182.56it/s]                        \n",
      "Episode 92: Total Reward = [149.], Loss = -20.7291: : 2000it [00:11, 176.03it/s]                        \n",
      "Episode 93: Total Reward = [-106.], Loss = 59.8826: : 1847it [00:10, 179.20it/s]                       \n",
      "Episode 94: Total Reward = [-116.], Loss = 29.4676: : 2000it [00:10, 191.57it/s]                        \n",
      "Episode 95: Total Reward = [511.], Loss = -120.1984: : 2000it [00:10, 187.57it/s]                        \n",
      "Episode 96: Total Reward = [-34.], Loss = 5.8834: : 2000it [00:10, 193.29it/s]                        \n",
      "Episode 97: Total Reward = [-114.], Loss = 27.4906: : 2000it [00:10, 192.92it/s]                        \n",
      "Episode 98: Total Reward = [-58.], Loss = 34.6096: : 2000it [00:10, 191.92it/s]                        \n",
      "Episode 99: Total Reward = [70.], Loss = -26.0693: : 2000it [00:10, 189.16it/s]                        \n",
      "Episode 100: Total Reward = [-10.], Loss = -5.7220: : 1751it [00:09, 177.64it/s]                       \n"
     ]
    }
   ],
   "source": [
    "num_episodes = 100 # Number of episodes to train for\n",
    "gamma = 0.99 # Discount factor for future rewards\n",
    "max_steps_per_episode = 1000\n",
    "batch_size = 256\n",
    "for episode in range(num_episodes):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    experiences = [] # List to store experiences\n",
    "\n",
    "    pbar = tqdm(total=max_steps_per_episode, desc=f\"Episode {episode+1}/{num_episodes}: training\")\n",
    "    \n",
    "    while not done and len(experiences) < max_steps_per_episode:\n",
    "        # Convert observation to tensor\n",
    "        obs_tensor = torch.tensor(obs, dtype=torch.float32, device=device).permute(0,3,1,2)\n",
    "        \n",
    "        # Forward pass through the model to get policy logits and value\n",
    "        policy_logits, value = model(obs_tensor)\n",
    "        \n",
    "        # Sample action from the policy\n",
    "        action_probs = torch.nn.functional.softmax(policy_logits, dim=-1)\n",
    "        action = torch.multinomial(action_probs, 1).item()\n",
    "        \n",
    "        # Take the action in the environment\n",
    "        next_obs, reward, done, info = env.step([action])\n",
    "        \n",
    "        # Store the experience for loss calculation\n",
    "        experiences.append((obs_tensor, action, reward, torch.tensor(next_obs, dtype=torch.float32).unsqueeze(0), done))\n",
    "        \n",
    "        obs = next_obs\n",
    "        \n",
    "        # Accumulating the reward\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # updating the tqdm loading\n",
    "        pbar.update(1)\n",
    "    loss = 0\n",
    "    # Calculate the loss and update the model\n",
    "    for obs, action, reward, next_obs, done in random.sample(experiences, batch_size):\n",
    "        # Forward pass through the model to get policy logits and value for the current state\n",
    "        policy_logits, value = model(obs)\n",
    "        \n",
    "        # Forward pass through the model to get policy logits and value for the next state\n",
    "        next_obs = next_obs.squeeze(0).permute(0,3,1,2).to(device=device)\n",
    "        next_policy_logits, next_value = model(next_obs)\n",
    "        \n",
    "        # Calculate the advantage\n",
    "        reward = torch.tensor(reward, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Assuming gamma is a scalar or a PyTorch tensor\n",
    "        gamma = torch.tensor(gamma, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Ensure next_value and value are tensors\n",
    "        next_value = torch.tensor(next_value, dtype=torch.float32, device=device)\n",
    "        value = torch.tensor(value, dtype=torch.float32, device=device)\n",
    "\n",
    "        advantage = reward + gamma * next_value - value\n",
    "        \n",
    "        # Calculate the PPO loss\n",
    "        old_log_probs = torch.nn.functional.log_softmax(policy_logits, dim=-1)\n",
    "        new_log_probs = torch.nn.functional.log_softmax(next_policy_logits, dim=-1)\n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantage\n",
    "        surr2 = torch.clamp(ratio, 1.0 - 0.2, 1.0 + 0.2) * advantage\n",
    "        loss += -torch.min(surr1, surr2) + 0.5 * nn.MSELoss()(value, next_value)\n",
    "        \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    # Perform a backward pass\n",
    "    loss.mean().backward()\n",
    "    # Update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # tqdm stuff updating\n",
    "    pbar.set_description(f\"Episode {episode+1}: Total Reward = {episode_reward}, Loss = {loss.mean():.4f}\")\n",
    "    pbar.update(1000)\n",
    "    pbar.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'CustomCNN.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Evaluating the RL Agents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation criteria \n",
    "def evaluate_agent(agent, episodes=5):\n",
    "    # list to store rewards\n",
    "    total_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        # resetting the env so the game start from beginning.\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        # while game is not ended\n",
    "        while not done:\n",
    "            try:\n",
    "                action, _ = agent.predict(state)\n",
    "            except:\n",
    "                action, _ = agent(torch.tensor(state, dtype=torch.float32).to(device).permute(0, 3, 1, 2))\n",
    "                action = [torch.argmax(action).item()]\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            # Commutative sum of reward\n",
    "            total_reward += reward\n",
    "        total_rewards.append(total_reward)\n",
    "        # total rewards per episode\n",
    "        print(f\"Episode {episode + 1}/{episodes}, Total Reward: {total_reward}\")\n",
    "    # average rewards over all episodes\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    print(f\"Average Reward: {avg_reward}\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Total Reward: [450.]\n",
      "Episode 2/5, Total Reward: [389.]\n",
      "Episode 3/5, Total Reward: [396.]\n",
      "Episode 4/5, Total Reward: [231.]\n",
      "Episode 5/5, Total Reward: [391.]\n",
      "Average Reward: 371.3999938964844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "371.4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO.load(\"CnnPolicy\")\n",
    "avg_cnn_reward = evaluate_agent(model)\n",
    "avg_cnn_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Total Reward: [411.]\n",
      "Episode 2/5, Total Reward: [402.]\n",
      "Episode 3/5, Total Reward: [403.]\n",
      "Episode 4/5, Total Reward: [397.]\n",
      "Episode 5/5, Total Reward: [451.]\n",
      "Average Reward: 412.79998779296875\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"MlpPolicy\")\n",
    "avg_mlp_reward = evaluate_agent(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/5, Total Reward: [330.]\n",
      "Episode 2/5, Total Reward: [330.]\n",
      "Episode 3/5, Total Reward: [330.]\n",
      "Episode 4/5, Total Reward: [330.]\n",
      "Episode 5/5, Total Reward: [330.]\n",
      "Average Reward: 330.0\n"
     ]
    }
   ],
   "source": [
    "model = CustomCNN(num_actions=env.action_space.n)\n",
    "\n",
    "# Load the model\n",
    "model.load_state_dict(torch.load('CustomCNN.pth'))\n",
    "model = model.to(\"mps\")\n",
    "model.eval()\n",
    "avg_CustomCnn_reward = evaluate_agent(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Testing it Out**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=245760, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=245760, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=64, out_features=7, bias=True)\n",
       "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading best model\n",
    "# model = PPO.load('MlpPolicy') if avg_mlp_reward > avg_cnn_reward else PPO.load(\"CnnPolicy\")\n",
    "model = PPO.load('MlpPolicy')\n",
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 13:26:03.296 Python[9832:882773] Warning: Window move completed without beginning\n"
     ]
    }
   ],
   "source": [
    "# Start the game \n",
    "state = env.reset()\n",
    "done = False\n",
    "# Loop through the game\n",
    "while not done: \n",
    "    # model prediction for action space\n",
    "    action, _ = model.predict(state)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    # rendering the game\n",
    "    env.render(mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mariobros",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
